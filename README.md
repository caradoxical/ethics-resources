<div id="top"></div>

<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown "reference style" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

<h3 align="center">AI Ethics Resources</h3>

  <p align="center">
    A collection of media, content, and additional resources related to AI and technology ethics
  </p>
</div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#websites">Websites</a></li>
    <li><a href="#talks">Talks</a></li>
    <li><a href="#podcasts-and-youtube-channels">Podcasts and YouTube Channels</a></li>
    <li><a href="#articles-and-white-papers">Articles and White Papers</a></li>
    <li><a href="#books">Books</a></li>
    <li><a href="#conferences-and-events">Conferences and Events</a></li>
    <li><a href="#published-research">Published Research</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>


## Websites

- [AI Ethicist](https://www.aiethicist.org/): a global repository of references and resources related to AI ethics
- [Cyberculture & Social Justice Directory](http://www.cybercultureandsocialjustice.com/): a directory of research in the broader field of cyber ethics
- [GRACE: Global Review of AI Community Ethics](https://ojs.stanford.edu/ojs/index.php/grace/about): peer-reviewed journal from Stanford University

<p align="right">(<a href="#top">back to top</a>)</p>

## Talks


## Podcasts and YouTube Channels


<p align="right">(<a href="#top">back to top</a>)</p>

## Articles and White Papers

- AI4People's [Ethical Framework for A Good AI Society: Opportunities, Risks, Principles, and Recommendations](https://www.eismd.eu/wp-content/uploads/2019/02/Ethical-Framework-for-a-Good-AI-Society.pdf)
- 
<p align="right">(<a href="#top">back to top</a>)</p>

## Books

- [The Oxford Handbook of Ethics of AI](https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780190067397.001.0001/oxfordhb-9780190067397)
- [Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way](https://philpapers.org/rec/DIGRAI) by Virginia Dignum
- [The Ethical Algorithm: The Science of Socially Aware Algorithm Design](https://www.adlibris.com/se/bok/the-ethical-algorithm-9780190948207?gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gbcVOhkNH_tjphsNP5wm1KznTWXHDkCJJIlri3aHXaAfEey8rGFyxgaAjPFEALw_wcB) by Michael Kearns
- [Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy](https://www.adlibris.com/se/bok/weapons-of-math-destruction-9780141985411?gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gYiPDTv9HX_sDhumMngHt7JBssR02P_TyPSrEaufGyNFn7VOHV8eKEaAgOKEALw_wcB) by Cathy O'Neil
- [Hello World: Being Human in the Age of Algorithms](https://www.adlibris.com/se/bok/hello-world-9781784163068?gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gZyQO-UQTMFtg77hbTcgIIlaU7WPDeuBVAYfks0NoTTHdb6BJKeTAsaAv_3EALw_wcB) by Hannah Fry
- [Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor](https://www.adlibris.com/se/bok/automating-inequality-how-high-tech-tools-profile-police-and-punish-the-poor-9781250215789) by Virginia Eubanks
- [Artificial Unintelligence: How Computers Misunderstand the World](https://mitpress.mit.edu/books/artificial-unintelligence) by Meredith Broussard
- [Algorithms of Oppression: How Search Engines Reinforce Racism](https://nyupress.org/9781479837243/algorithms-of-oppression/) by Safiya Umoja Noble
- [Race After Technology: Abolitionist Tools for the New Jim Code](https://www.adlibris.com/se/bok/race-after-technology-9781509526406?gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gZPHPHgrM_z8cxNWYd1iweWrN21s2NhYVr_8UlRF0xF2prNyOjgugEaAgLaEALw_wcB) by Ruha Benjamin
- [Technically Wrong: Sexist Apps, Biased Algorithms, and Other Threats of Toxic Tech](https://www.adlibris.com/se/bok/technically-wrong-9780393356045?gclid=Cj0KCQiA_8OPBhDtARIsAKQu0gbNvurfR3q9w33ydP9E8eQVLDJ8L84BHnAiPZOoEWHBvzxsRGZrkVQaAgt2EALw_wcB) by Sara Wachter-Boettcher

<p align="right">(<a href="#top">back to top</a>)</p>

## Conferences and Events

- AAAI/ACM conference on [Artificial Intelligence, Ethics, and Society (AIES)](https://www.aies-conference.com/2022/)
-

<p align="right">(<a href="#top">back to top</a>)</p>

## Published Research

*Note: this is not an exhaustive list by any means and reflects some of my own research interests. Feel free to request additional Key Topics tags!*

| Authors | Title | Year | Key Topics |
| ------- | ----- | ---- | ---------- |
| Aroyo et al. | [Data Excellence for AI: Why Should You Care](https://arxiv.org/ftp/arxiv/papers/2111/2111.10391.pdf) | 2021 | `Data` |
| Aroyo & Welty | [Truth is a Lie: Crowd Truth and the Seven Myths of Human Annotation](https://ojs.aaai.org/index.php/aimagazine/article/view/2564) | 2015 | `Data` `NLP` |
| Bartl et al. | [Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias](https://arxiv.org/pdf/2010.14534.pdf) | 2020 | `NLP` `LLM` `Bias Mitigation` | 
| Basta et al. | [Evaluating the Underlying Gender Bias in Contextualized Word Embeddings](https://aclanthology.org/W19-3805.pdf) | 2019 | `NLP` `LLM` | 
| Bender et al. | [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922) | 2021 | `NLP` `LLM` |
| Dillon, S. | [The Eliza Effect and Its Dangers: From Demystification to Gender Critique](https://www.repository.cam.ac.uk/bitstream/handle/1810/304211/Dillon%20The%20Eliza%20Effect%20JCR.pdf?sequence=1&isAllowed=n) | 2020 | `Virtual Assistants` `Feminism` |
| Fazelpour & De-Arteaga | [Diversity in Sociotechnical Machine Learning Systems](https://arxiv.org/pdf/2107.09163.pdf) | 2021 | `Algorithmic Fairness` | 
| Gebru et al. | [Datasheets for Datasets](https://arxiv.org/pdf/1803.09010.pdf) | 2021 | `Data` `XAI` |
| Gonen & Goldberg | [Lipstick on a Pig: Debiasing Methods Cover up Systemic Gender Biases in Word Embeddings But Do Not Remove Them](https://arxiv.org/pdf/1903.03862.pdf) | 2019 | `NLP` `LLM` |
| Guo & Caliskan | [Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases](https://dl.acm.org/doi/pdf/10.1145/3461702.3462536) | 2021 | `NLP` `LLM` |
| Hanna et al. | [Towards a Critical Race Methodology in Algorithmic Fairness](https://arxiv.org/pdf/1912.03593.pdf) | 2019 | `Critical Race Theory` `Algorithmic Fairness` |
| Hardt et al. | [Equality of Opportunity in Supervised Learning](https://arxiv.org/pdf/1610.02413.pdf) | 2016 | `Supervised Learning` `Algorithmic Fairness` |
| Hutchinson et al. | [Towards Accountability for Machine Learning Datasets: Practices from Sofftware Engineering and Infrastructure](https://arxiv.org/pdf/2010.13561.pdf) | 2021 | `Data` |
| Jobin et al. | [The global landscape of AI ethics](https://www.nature.com/articles/s42256-019-0088-2) | 2019 | `Literature Review` |
| Kurita et al. | [Measuring Bias in Contextualized Word Representations](https://aclanthology.org/W19-3823.pdf) | 2019 | `NLP` `LLM` |
| Lee, M. | [Understanding Perceptions of Algorithmic Decisions: Fairness, Trust, and Emotion in Response to Algorithmic Management](https://journals.sagepub.com/doi/full/10.1177/2053951718756684) | 2018 | `Algorithmic Fairness` |
| Matthews et al. | [Gender Bias in Natural Language Processing Across Human Languages](https://aclanthology.org/2021.trustnlp-1.6.pdf) | 2021 | `NLP` `LLM` | 
| Mitchell et al. | [Model Cards for Model Reporting](https://arxiv.org/pdf/1810.03993.pdf) | 2019 | `Data` `XAI` | 
| Nadeem et al. | [StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/pdf/2004.09456.pdf) | 2020 | `NLP` `LLM` `Datasets` |
| Nangia et al. | [CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://arxiv.org/pdf/2010.00133.pdf) | 2020 | `NLP` `LLM` `Datasets` |
| Selbst et al. | [Fairness and Abstraction in Sociotechnical Systems](https://dl.acm.org/doi/10.1145/3287560.3287598) | 2019 | `Algorithmic Fairness` |
| Suresh et al. | [A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle](https://arxiv.org/pdf/1901.10002.pdf) | 2021 | `Algorithmic Fairness` 
| Webster et al. | [Measuring and Reducing Gendered Correlations in Pre-trained Models](https://arxiv.org/pdf/2010.06032.pdf) | 2020 | `NLP` `Bias Mitigation` `LLM` |



<p align="right">(<a href="#top">back to top</a>)</p>

## Contact

Jesse Shanahan - [@enceladosaurus](https://twitter.com/enceladosaurus) - jess.c.shanahan@gmail.com

<p align="right">(<a href="#top">back to top</a>)</p>


<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/enceladosaurus/ethics-resources.svg?style=for-the-badge
[contributors-url]: https://github.com/enceladosaurus/ethics-resources/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/enceladosaurus/ethics-resources.svg?style=for-the-badge
[forks-url]: https://github.com/enceladosaurus/ethics-resources/network/members
[stars-shield]: https://img.shields.io/github/stars/enceladosaurus/ethics-resources.svg?style=for-the-badge
[stars-url]: https://github.com/enceladosaurus/ethics-resources/stargazers
[issues-shield]: https://img.shields.io/github/issues/enceladosaurus/ethics-resources.svg?style=for-the-badge
[issues-url]: https://github.com/enceladosaurus/ethics-resources/issues
